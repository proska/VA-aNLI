###### ModelArguments
model_name_or_path:
model_type:
config_name:
tokenizer_name: 
cache_dir: 

###### DataProcessingArguments
task_name:
data_dir:
max_seq_length: 128
overwrite_cache: false

###### ATArgs
adv_train:
adv_steps:
adv_init_mag:
adv_max_norm:
adv_lr:
data_size: 0
vocab_size: 30522
hidden_size: 768
evaluate_during_training: true
select_pairs: true
mode: "train"
treshhold: 0.3
k: 2
save_step: 500
local_rank1: 2
lamda: 10.0
per_device_eval_batch_size: 5
add_constraint: true


#####
output_dir: 
overwrite_output_dir: false,
do_train: false
do_eval: 
do_predict: false
evaluation_strategy: "no"
prediction_loss_only: false
per_device_train_batch_size: 8
per_gpu_train_batch_size: 
per_gpu_eval_batch_size: 

gradient_accumulation_steps: 1
eval_accumulation_steps: 
learning_rate: 5e-5
weight_decay: 0.0
adam_beta1: 0.9
adam_beta2: 0.999
adam_epsilon: 1e-8
max_grad_norm: 1.0

num_train_epochs: 3.0
max_steps: -1
lr_scheduler_type: "linear"
warmup_steps: 0

logging_dir: runs
logging_first_step: false
logging_steps: 500
save_steps: 500
save_total_limit: 
no_cuda: false
seed: 57

fp16: false
fp16_opt_level: "O1"
fp16_backend: "auto"
local_rank: -1

tpu_num_cores: 
tpu_metrics_debug: false
debug: false

dataloader_drop_last: false
eval_steps: 
dataloader_num_workers: 0

past_index: -1

run_name: 
disable_tqdm: 

remove_unused_columns: true
label_names: 

load_best_model_at_end: false
metric_for_best_model: 
greater_is_better: 
ignore_data_skip: false
sharded_ddp: false
deepspeed: 
label_smoothing_factor: 0.0
adafactor: false
group_by_length: false
report_to: 
ddp_find_unused_parameters: 
dataloader_pin_memory: true
n_gpu: false
_n_gpu: false

device:

user:
name:
hydra:
  run:
    dir: /nas/home/${user}/VA-aNLI/Output/${hydra.job.name}


#####